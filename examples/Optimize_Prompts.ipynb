{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Prompts\n",
    "\n",
    "Crafting effective prompts is a critical skill when working with AI models. Even experienced users can inadvertently introduce contradictions, ambiguities, or inconsistencies that lead to suboptimal results. The system demonstrated here helps identify and fix common issues, resulting in more reliable and effective prompts.\n",
    "\n",
    "The optimization process uses a multi-agent approach with specialized AI agents collaborating to analyze and rewrite prompts. The system automatically identifies and addresses several types of common issues:\n",
    "\n",
    "- **Contradictions** in the prompt instructions\n",
    "- Missing or unclear **format specifications**\n",
    "- **Inconsistencies** between the prompt and few-shot examples\n",
    "\n",
    "---\n",
    "\n",
    "**Objective**: This cookbook demonstrates best practices for using Agents SDK together with Evals to build an early version of OpenAI's prompt optimization system. You can optimize your prompt using this code or use the optimizer [in our playground!](https://platform.openai.com/playground/prompts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ask ChatGPT\n",
    "\n",
    "\n",
    "**Cookbook Structure**  \n",
    "This notebook follows this structure:\n",
    "\n",
    "- [Step 1. System Overview](#1-system-overview) - Learn how the prompt optimization system works  \n",
    "- [Step 2. Data Models](#2-data-models) - Understand the data structures used by the system\n",
    "- [Step 3. Defining the Agents](#3-defining-the-agents) - Look at agents that analyze and improve prompts\n",
    "- [Step 4. Evaluations](#4-using-evaluations-to-arrive-at-these-agents) - Use Evals to verify our agent model choice and instructions\n",
    "- [Step 5. Run Optimization Workflow](#4-run-optimization-workflow) - See how the workflow hands off the prompts\n",
    "- [Step 6. Examples](#5-examples) - Explore real-world examples of prompt optimization\n",
    "\n",
    "**Prerequisites**\n",
    "- The `openai` Python package \n",
    "- The `openai-agents` package\n",
    "- An OpenAI API key set as `OPENAI_API_KEY` in your environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Overview\n",
    "\n",
    "The prompt optimization system uses a collaborative multi-agent approach to analyze and improve prompts. Each agent specializes in either detecting or rewriting a specific type of issue:\n",
    "\n",
    "1. **Dev-Contradiction-Checker**: Scans the prompt for logical contradictions or impossible instructions, like \"only use positive numbers\" and \"include negative examples\" in the same prompt.\n",
    "\n",
    "2. **Format-Checker**: Identifies when a prompt expects structured output (like JSON, CSV, or Markdown) but fails to clearly specify the exact format requirements. This agent ensures that all necessary fields, data types, and formatting rules are explicitly defined.\n",
    "\n",
    "3. **Few-Shot-Consistency-Checker**: Examines example conversations to ensure that the assistant's responses actually follow the rules specified in the prompt. This catches mismatches between what the prompt requires and what the examples demonstrate.\n",
    "\n",
    "4. **Dev-Rewriter**: After issues are identified, this agent rewrites the prompt to resolve contradictions and clarify format specifications while preserving the original intent.\n",
    "\n",
    "5. **Few-Shot-Rewriter**: Updates inconsistent example responses to align with the rules in the prompt, ensuring all examples properly comply with the new developer prompt.\n",
    "\n",
    "By working together, these agents can systematically identify and fix issues in prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Any, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from agents import Agent, Runner, set_default_openai_client, trace\n",
    "\n",
    "openai_client: AsyncOpenAI | None = None\n",
    "\n",
    "def _get_openai_client() -> AsyncOpenAI:\n",
    "    global openai_client\n",
    "    if openai_client is None:\n",
    "        openai_client = AsyncOpenAI(\n",
    "            api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key\"),\n",
    "        )\n",
    "    return openai_client\n",
    "\n",
    "set_default_openai_client(_get_openai_client())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Models\n",
    "\n",
    "To facilitate structured communication between agents, the system uses Pydantic models to define the expected format for inputs and outputs. These Pydantic models help validate data and ensure consistency throughout the workflow.\n",
    "\n",
    "The data models include:\n",
    "\n",
    "1. **Role** - An enumeration for message roles (user/assistant)\n",
    "2. **ChatMessage** - Represents a single message in a conversation\n",
    "3. **Issues** - Base model for reporting detected issues\n",
    "4. **FewShotIssues** - Extended model that adds rewrite suggestions for example messages\n",
    "5. **MessagesOutput** - Contains optimized conversation messages\n",
    "6. **DevRewriteOutput** - Contains the improved developer prompt\n",
    "\n",
    "Using Pydantic allows the system to validate that all data conforms to the expected format at each step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Role(str, Enum):\n",
    "    \"\"\"Role enum for chat messages.\"\"\"\n",
    "    user = \"user\"\n",
    "    assistant = \"assistant\"\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    \"\"\"Single chat message used in few-shot examples.\"\"\"\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "class Issues(BaseModel):\n",
    "    \"\"\"Structured output returned by checkers.\"\"\"\n",
    "    has_issues: bool\n",
    "    issues: List[str]\n",
    "    \n",
    "    @classmethod\n",
    "    def no_issues(cls) -> \"Issues\":\n",
    "        return cls(has_issues=False, issues=[])\n",
    "\n",
    "class FewShotIssues(Issues):\n",
    "    \"\"\"Output for few-shot contradiction detector including optional rewrite suggestions.\"\"\"\n",
    "    rewrite_suggestions: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    @classmethod\n",
    "    def no_issues(cls) -> \"FewShotIssues\":\n",
    "        return cls(has_issues=False, issues=[], rewrite_suggestions=[])\n",
    "\n",
    "class MessagesOutput(BaseModel):\n",
    "    \"\"\"Structured output returned by `rewrite_messages_agent`.\"\"\"\n",
    "\n",
    "    messages: list[ChatMessage]\n",
    "\n",
    "\n",
    "class DevRewriteOutput(BaseModel):\n",
    "    \"\"\"Rewriter returns the cleaned-up developer prompt.\"\"\"\n",
    "\n",
    "    new_developer_message: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Agents\n",
    "\n",
    "In this section, we create specialized AI agents using the `Agent` class from the `openai-agents` package. Looking at these agent definitions reveals several best practices for creating effective AI instructions:\n",
    "\n",
    "### Best Practices in Agent Instructions\n",
    "\n",
    "1. **Clear Scope Definition**: Each agent has a narrowly defined purpose with explicit boundaries. For example, the contradiction checker focuses only on \"genuine self-contradictions\" and explicitly states that \"overlaps or redundancies are not contradictions.\"\n",
    "\n",
    "2. **Step-by-Step Process**: Instructions provide a clear methodology, like how the format checker first categorizes the task before analyzing format requirements.\n",
    "\n",
    "3. **Explicit Definitions**: Key terms are defined precisely to avoid ambiguity. The few-shot consistency checker includes a detailed \"Compliance Rubric\" explaining exactly what constitutes compliance.\n",
    "\n",
    "4. **Boundary Setting**: Instructions specify what the agent should NOT do. The few-shot checker explicitly lists what's \"Out-of-scope\" to prevent over-flagging issues.\n",
    "\n",
    "5. **Structured Output Requirements**: Each agent has a strictly defined output format with examples, ensuring consistency in the optimization pipeline.\n",
    "\n",
    "These principles create reliable, focused agents that work effectively together in the optimization system. Below we see the complete agent definitions with their detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dev_contradiction_checker = Agent(\n",
    "    name=\"contradiction_detector\",\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=Issues,\n",
    "    instructions=\"\"\"\n",
    "    You are **Dev-Contradiction-Checker**.\n",
    "\n",
    "    Goal\n",
    "    Detect *genuine* self-contradictions or impossibilities **inside** the developer prompt supplied in the variable `DEVELOPER_MESSAGE`.\n",
    "\n",
    "    Definitions\n",
    "    • A contradiction = two clauses that cannot both be followed.\n",
    "    • Overlaps or redundancies in the DEVELOPER_MESSAGE are *not* contradictions.\n",
    "\n",
    "    What you MUST do\n",
    "    1. Compare every imperative / prohibition against all others.\n",
    "    2. List at most FIVE contradictions (each as ONE bullet).\n",
    "    3. If no contradiction exists, say so.\n",
    "\n",
    "    Output format (**strict JSON**)\n",
    "    Return **only** an object that matches the `Issues` schema:\n",
    "\n",
    "    ```json\n",
    "    {\"has_issues\": <bool>,\n",
    "    \"issues\": [\n",
    "        \"<bullet 1>\",\n",
    "        \"<bullet 2>\"\n",
    "    ]\n",
    "    }\n",
    "    - has_issues = true IFF the issues array is non-empty.\n",
    "    - Do not add extra keys, comments or markdown.\n",
    "\"\"\",\n",
    ")\n",
    "format_checker = Agent(\n",
    "    name=\"format_checker\",\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=Issues,\n",
    "    instructions=\"\"\"\n",
    "    You are Format-Checker.\n",
    "\n",
    "    Task\n",
    "    Decide whether the developer prompt requires a structured output (JSON/CSV/XML/Markdown table, etc.).\n",
    "    If so, flag any missing or unclear aspects of that format.\n",
    "\n",
    "    Steps\n",
    "    Categorise the task as:\n",
    "    a. \"conversation_only\", or\n",
    "    b. \"structured_output_required\".\n",
    "\n",
    "    For case (b):\n",
    "    - Point out absent fields, ambiguous data types, unspecified ordering, or missing error-handling.\n",
    "\n",
    "    Do NOT invent issues if unsure. be a little bit more conservative in flagging format issues\n",
    "\n",
    "    Output format\n",
    "    Return strictly-valid JSON following the Issues schema:\n",
    "\n",
    "    {\n",
    "    \"has_issues\": <bool>,\n",
    "    \"issues\": [\"<desc 1>\", \"...\"]\n",
    "    }\n",
    "    Maximum five issues. No extra keys or text.\n",
    "\"\"\",\n",
    ")\n",
    "fewshot_consistency_checker = Agent(\n",
    "    name=\"fewshot_consistency_checker\",\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=FewShotIssues,\n",
    "    instructions=\"\"\"\n",
    "    You are FewShot-Consistency-Checker.\n",
    "\n",
    "    Goal\n",
    "    Find conflicts between the DEVELOPER_MESSAGE rules and the accompanying **assistant** examples.\n",
    "\n",
    "    USER_EXAMPLES:      <all user lines>          # context only\n",
    "    ASSISTANT_EXAMPLES: <all assistant lines>     # to be evaluated\n",
    "\n",
    "    Method\n",
    "    Extract key constraints from DEVELOPER_MESSAGE:\n",
    "    - Tone / style\n",
    "    - Forbidden or mandated content\n",
    "    - Output format requirements\n",
    "\n",
    "    Compliance Rubric - read carefully\n",
    "    Evaluate only what the developer message makes explicit.\n",
    "\n",
    "    Objective constraints you must check when present:\n",
    "    - Required output type syntax (e.g., \"JSON object\", \"single sentence\", \"subject line\").\n",
    "    - Hard limits (length ≤ N chars, language required to be English, forbidden words, etc.).\n",
    "    - Mandatory tokens or fields the developer explicitly names.\n",
    "\n",
    "    Out-of-scope (DO NOT FLAG):\n",
    "    - Whether the reply \"sounds generic\", \"repeats the prompt\", or \"fully reflects the user's request\" - unless the developer text explicitly demands those qualities.\n",
    "    - Creative style, marketing quality, or depth of content unless stated.\n",
    "    - Minor stylistic choices (capitalisation, punctuation) that do not violate an explicit rule.\n",
    "\n",
    "    Pass/Fail rule\n",
    "    - If an assistant reply satisfies all objective constraints, it is compliant, even if you personally find it bland or loosely related.\n",
    "    - Only record an issue when a concrete, quoted rule is broken.\n",
    "\n",
    "    Empty assistant list ⇒ immediately return has_issues=false.\n",
    "\n",
    "    For each assistant example:\n",
    "    - USER_EXAMPLES are for context only; never use them to judge compliance.\n",
    "    - Judge each assistant reply solely against the explicit constraints you extracted from the developer message.\n",
    "    - If a reply breaks a specific, quoted rule, add a line explaining which rule it breaks.\n",
    "    - Optionally, suggest a rewrite in one short sentence (add to rewrite_suggestions).\n",
    "    - If you are uncertain, do not flag an issue.\n",
    "    - Be conservative—uncertain or ambiguous cases are not issues.\n",
    "\n",
    "    be a little bit more conservative in flagging few shot contradiction issues\n",
    "    Output format\n",
    "    Return JSON matching FewShotIssues:\n",
    "\n",
    "    {\n",
    "    \"has_issues\": <bool>,\n",
    "    \"issues\": [\"<explanation 1>\", \"...\"],\n",
    "    \"rewrite_suggestions\": [\"<suggestion 1>\", \"...\"] // may be []\n",
    "    }\n",
    "    List max five items for both arrays.\n",
    "    Provide empty arrays when none.\n",
    "    No markdown, no extra keys.\n",
    "    \"\"\",\n",
    ")\n",
    "dev_rewriter = Agent(\n",
    "    name=\"dev_rewriter\",\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=DevRewriteOutput,\n",
    "    instructions=\"\"\"\n",
    "    You are Dev-Rewriter.\n",
    "\n",
    "    You receive:\n",
    "    - ORIGINAL_DEVELOPER_MESSAGE\n",
    "    - CONTRADICTION_ISSUES (may be empty)\n",
    "    - FORMAT_ISSUES (may be empty)\n",
    "\n",
    "    Rewrite rules\n",
    "    Preserve the original intent and capabilities.\n",
    "\n",
    "    Resolve each contradiction:\n",
    "    - Keep the clause that preserves the message intent; remove/merge the conflicting one.\n",
    "\n",
    "    If FORMAT_ISSUES is non-empty:\n",
    "    - Append a new section titled ## Output Format that clearly defines the schema or gives an explicit example.\n",
    "\n",
    "    Do NOT change few-shot examples.\n",
    "\n",
    "    Do NOT add new policies or scope.\n",
    "\n",
    "    Output format (strict JSON)\n",
    "    {\n",
    "    \"new_developer_message\": \"<full rewritten text>\"\n",
    "    }\n",
    "    No other keys, no markdown.\n",
    "\"\"\",\n",
    ")\n",
    "fewshot_rewriter = Agent(\n",
    "    name=\"fewshot_rewriter\",\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=MessagesOutput,\n",
    "    instructions=\"\"\"\n",
    "    You are FewShot-Rewriter.\n",
    "\n",
    "    Input payload\n",
    "    - NEW_DEVELOPER_MESSAGE (already optimized)\n",
    "    - ORIGINAL_MESSAGES (list of user/assistant dicts)\n",
    "    - FEW_SHOT_ISSUES (non-empty)\n",
    "\n",
    "    Task\n",
    "    Regenerate only the assistant parts that were flagged.\n",
    "    User messages must remain identical.\n",
    "    Every regenerated assistant reply MUST comply with NEW_DEVELOPER_MESSAGE.\n",
    "\n",
    "    After regenerating each assistant reply, verify:\n",
    "    - It matches NEW_DEVELOPER_MESSAGE. ENSURE THAT THIS IS TRUE.\n",
    "\n",
    "    Output format\n",
    "    Return strict JSON that matches the MessagesOutput schema:\n",
    "\n",
    "    {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\"}\n",
    "        ]\n",
    "    }\n",
    "    Guidelines\n",
    "    - Preserve original ordering and total count.\n",
    "    - If a message was unproblematic, copy it unchanged.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Evaluations to Arrive at These Agents\n",
    "\n",
    "Let's see how we used OpenAI Evals to tune agent instructions and pick the correct model to use. In order to do so we constructed a set of golden examples: each one contains original messages (developer message + user/assistant message) and the changes our optimization workflow should make. Here are two example of golden pairs that we used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[\n",
    "  {\n",
    "    \"focus\": \"contradiction_issues\",\n",
    "    \"input_payload\": {\n",
    "      \"developer_message\": \"Always answer in **English**.\\nNunca respondas en inglés.\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"¿Qué hora es?\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"golden_output\": {\n",
    "      \"changes\": true,\n",
    "      \"new_developer_message\": \"Always answer **in English**.\",\n",
    "      \"new_messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"¿Qué hora es?\"\n",
    "        }\n",
    "      ],\n",
    "      \"contradiction_issues\": \"Developer message simultaneously insists on English and forbids it.\",\n",
    "      \"few_shot_contradiction_issues\": \"\",\n",
    "      \"format_issues\": \"\",\n",
    "      \"general_improvements\": \"\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"focus\": \"few_shot_contradiction_issues\",\n",
    "    \"input_payload\": {\n",
    "      \"developer_message\": \"Respond with **only 'yes' or 'no'** – no explanations.\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Is the sky blue?\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"Yes, because wavelengths …\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Is water wet?\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"Yes.\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"golden_output\": {\n",
    "      \"changes\": true,\n",
    "      \"new_developer_message\": \"Respond with **only** the single word \\\"yes\\\" or \\\"no\\\".\",\n",
    "      \"new_messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Is the sky blue?\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"yes\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Is water wet?\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": \"yes\"\n",
    "        }\n",
    "      ],\n",
    "      \"contradiction_issues\": \"\",\n",
    "      \"few_shot_contradiction_issues\": \"Assistant examples include explanations despite instruction not to.\",\n",
    "      \"format_issues\": \"\",\n",
    "      \"general_improvements\": \"\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these 20 hand labelled golden outputs which cover a range of contradiction issues, few shot issues, format issues, no issues, or a combination of issues, we built a python string check grader to verify two things: whether an issue was detected for each golden pair and whether the detected issue matched the expected one. From this signal, we tuned the agent instructions and which model to use to maximize our accuracy across this evaluation. We landed on the 4.1 model as a balance between accuracy, cost, and speed. The specific prompts we used also follow the 4.1 prompting guide. As you can see, we achieve the correct labels on all 20 golden outputs: identifying the right issues and avoiding false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy for the golden set](../images/optimizepromptfig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluation for the golden set](../images/optimizepromptfig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Optimization Workflow\n",
    "\n",
    "Let's dive into how the optimization system actually works end to end. The core workflow consists of multiple runs of the agents in parallel to efficiently process and optimize prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_messages(messages: List[Any]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Convert list of pydantic message models to JSON-serializable dicts.\"\"\"\n",
    "    result = []\n",
    "    for m in messages:\n",
    "        if hasattr(m, \"model_dump\"):\n",
    "            result.append(m.model_dump())\n",
    "        elif isinstance(m, dict) and \"role\" in m and \"content\" in m:\n",
    "            result.append({\"role\": str(m[\"role\"]), \"content\": str(m[\"content\"])})\n",
    "    return result\n",
    "\n",
    "async def optimize_prompt_parallel(\n",
    "    developer_message: str,\n",
    "    messages: List[\"ChatMessage\"],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs contradiction, format, and few-shot checkers in parallel,\n",
    "    then rewrites the prompt/examples if needed.\n",
    "    Returns a unified dict suitable for an API or endpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    with trace(\"optimize_prompt_workflow\"):\n",
    "        # 1. Run all checkers in parallel (contradiction, format, fewshot if there are examples)\n",
    "        tasks = [\n",
    "            Runner.run(dev_contradiction_checker, developer_message),\n",
    "            Runner.run(format_checker, developer_message),\n",
    "        ]\n",
    "        if messages:\n",
    "            fs_input = {\n",
    "                \"DEVELOPER_MESSAGE\": developer_message,\n",
    "                \"USER_EXAMPLES\": [m.content for m in messages if m.role == \"user\"],\n",
    "                \"ASSISTANT_EXAMPLES\": [m.content for m in messages if m.role == \"assistant\"],\n",
    "            }\n",
    "            tasks.append(Runner.run(fewshot_consistency_checker, json.dumps(fs_input)))\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Unpack results\n",
    "        cd_issues: Issues = results[0].final_output\n",
    "        fi_issues: Issues = results[1].final_output\n",
    "        fs_issues: FewShotIssues = results[2].final_output if messages else FewShotIssues.no_issues()\n",
    "\n",
    "        # 3. Rewrites as needed\n",
    "        final_prompt = developer_message\n",
    "        if cd_issues.has_issues or fi_issues.has_issues:\n",
    "            pr_input = {\n",
    "                \"ORIGINAL_DEVELOPER_MESSAGE\": developer_message,\n",
    "                \"CONTRADICTION_ISSUES\": cd_issues.model_dump(),\n",
    "                \"FORMAT_ISSUES\": fi_issues.model_dump(),\n",
    "            }\n",
    "            pr_res = await Runner.run(dev_rewriter, json.dumps(pr_input))\n",
    "            final_prompt = pr_res.final_output.new_developer_message\n",
    "\n",
    "        final_messages: list[ChatMessage] | list[dict[str, str]] = messages\n",
    "        if fs_issues.has_issues:\n",
    "            mr_input = {\n",
    "                \"NEW_DEVELOPER_MESSAGE\": final_prompt,\n",
    "                \"ORIGINAL_MESSAGES\": _normalize_messages(messages),\n",
    "                \"FEW_SHOT_ISSUES\": fs_issues.model_dump(),\n",
    "            }\n",
    "            mr_res = await Runner.run(fewshot_rewriter, json.dumps(mr_input))\n",
    "            final_messages = mr_res.final_output.messages\n",
    "\n",
    "        return {\n",
    "            \"changes\": True,\n",
    "            \"new_developer_message\": final_prompt,\n",
    "            \"new_messages\": _normalize_messages(final_messages),\n",
    "            \"contradiction_issues\": \"\\n\".join(cd_issues.issues),\n",
    "            \"few_shot_contradiction_issues\": \"\\n\".join(fs_issues.issues),\n",
    "            \"format_issues\": \"\\n\".join(fi_issues.issues),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Trace for the workflow](../images/optimizepromptfig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Optimization Workflow\n",
    "\n",
    "The `optimize_prompt_parallel` function implements a workflow to maximize efficiency through parallelization:\n",
    "\n",
    "1. **Parallel Issue Detection**: The first phase runs all checker agents simultaneously:\n",
    "   - `dev_contradiction_checker` searches for logical contradictions in the prompt\n",
    "   - `format_checker` looks for unclear format specifications\n",
    "   - `fewshot_consistency_checker` (if examples exist) checks for mismatches between the prompt and examples\n",
    "\n",
    "After the parallel checking phase, the workflow handles dependencies carefully:\n",
    "\n",
    "2. **Prompt Rewriting (Conditional)**: The `dev_rewriter` agent only runs if contradiction or format issues were detected. This agent depends on the outputs from:\n",
    "   - `dev_contradiction_checker` (the `cd_issues` variable)\n",
    "   - `format_checker` (the `fi_issues` variable)\n",
    "\n",
    "3. **Example Rewriting (Conditional)**: The `fewshot_rewriter` agent only runs if example inconsistencies were detected. This agent depends on:\n",
    "   - The rewritten prompt (must be done after prompt rewriting)\n",
    "   - The original messages\n",
    "   - The few-shot issues (the `fs_issues` variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examples\n",
    "\n",
    "Let's see the optimization system in action with some practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Fixing Contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}. (request_id: req_a914500705c6650f782d330ebc655b00)\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mnew_developer_message\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Run the example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m example_contradiction()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mexample_contradiction\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexample_contradiction\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# A prompt with contradictory instructions\u001b[39;00m\n\u001b[32m      3\u001b[39m     prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33mQuick-Start Card — Product Parser\u001b[39m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33mGoal  \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbrand\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msku\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:0,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcurrency\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUSD\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m},\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:[\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msizes\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:[],\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmaterials\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:[],\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcare_instructions\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:[]}\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[33mNote: It is acceptable to output null for any missing field instead of an error ###\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m optimize_prompt_parallel(prompt, [])\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mcontradiction_issues\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36moptimize_prompt_parallel\u001b[39m\u001b[34m(developer_message, messages)\u001b[39m\n\u001b[32m     28\u001b[39m     fs_input = {\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDEVELOPER_MESSAGE\u001b[39m\u001b[33m\"\u001b[39m: developer_message,\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUSER_EXAMPLES\u001b[39m\u001b[33m\"\u001b[39m: [m.content \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mASSISTANT_EXAMPLES\u001b[39m\u001b[33m\"\u001b[39m: [m.content \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     32\u001b[39m     }\n\u001b[32m     33\u001b[39m     tasks.append(Runner.run(fewshot_consistency_checker, json.dumps(fs_input)))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n\u001b[32m     38\u001b[39m cd_issues: Issues = results[\u001b[32m0\u001b[39m].final_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:295\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    248\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    296\u001b[39m     starting_agent,\n\u001b[32m    297\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    298\u001b[39m     context=context,\n\u001b[32m    299\u001b[39m     max_turns=max_turns,\n\u001b[32m    300\u001b[39m     hooks=hooks,\n\u001b[32m    301\u001b[39m     run_config=run_config,\n\u001b[32m    302\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    303\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    304\u001b[39m     session=session,\n\u001b[32m    305\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:545\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    540\u001b[39m logger.debug(\n\u001b[32m    541\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    542\u001b[39m )\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    546\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    547\u001b[39m             starting_agent,\n\u001b[32m    548\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    549\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    550\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    551\u001b[39m             context_wrapper,\n\u001b[32m    552\u001b[39m         ),\n\u001b[32m    553\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    554\u001b[39m             agent=current_agent,\n\u001b[32m    555\u001b[39m             all_tools=all_tools,\n\u001b[32m    556\u001b[39m             original_input=original_input,\n\u001b[32m    557\u001b[39m             generated_items=generated_items,\n\u001b[32m    558\u001b[39m             hooks=hooks,\n\u001b[32m    559\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    560\u001b[39m             run_config=run_config,\n\u001b[32m    561\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    562\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    563\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    564\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    565\u001b[39m         ),\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    568\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    569\u001b[39m         agent=current_agent,\n\u001b[32m    570\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         conversation_id=conversation_id,\n\u001b[32m    580\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:1235\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1233\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1235\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1236\u001b[39m     agent,\n\u001b[32m   1237\u001b[39m     system_prompt,\n\u001b[32m   1238\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1239\u001b[39m     output_schema,\n\u001b[32m   1240\u001b[39m     all_tools,\n\u001b[32m   1241\u001b[39m     handoffs,\n\u001b[32m   1242\u001b[39m     hooks,\n\u001b[32m   1243\u001b[39m     context_wrapper,\n\u001b[32m   1244\u001b[39m     run_config,\n\u001b[32m   1245\u001b[39m     tool_use_tracker,\n\u001b[32m   1246\u001b[39m     previous_response_id,\n\u001b[32m   1247\u001b[39m     conversation_id,\n\u001b[32m   1248\u001b[39m     prompt_config,\n\u001b[32m   1249\u001b[39m )\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1252\u001b[39m     agent=agent,\n\u001b[32m   1253\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1262\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1263\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/run.py:1483\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, previous_response_id, conversation_id, prompt_config)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them before the LLM call\u001b[39;00m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1470\u001b[39m     hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n\u001b[32m   1471\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1480\u001b[39m     ),\n\u001b[32m   1481\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1484\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1486\u001b[39m     model_settings=model_settings,\n\u001b[32m   1487\u001b[39m     tools=all_tools,\n\u001b[32m   1488\u001b[39m     output_schema=output_schema,\n\u001b[32m   1489\u001b[39m     handoffs=handoffs,\n\u001b[32m   1490\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1491\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1492\u001b[39m     ),\n\u001b[32m   1493\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1494\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1495\u001b[39m     prompt=prompt_config,\n\u001b[32m   1496\u001b[39m )\n\u001b[32m   1498\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/models/openai_responses.py:90\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     91\u001b[39m             system_instructions,\n\u001b[32m     92\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     93\u001b[39m             model_settings,\n\u001b[32m     94\u001b[39m             tools,\n\u001b[32m     95\u001b[39m             output_schema,\n\u001b[32m     96\u001b[39m             handoffs,\n\u001b[32m     97\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m     98\u001b[39m             conversation_id=conversation_id,\n\u001b[32m     99\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    100\u001b[39m             prompt=prompt,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    104\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/agents/models/openai_responses.py:305\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    306\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    307\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(conversation_id),\n\u001b[32m    308\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    309\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    310\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    311\u001b[39m     include=include,\n\u001b[32m    312\u001b[39m     tools=converted_tools_payload,\n\u001b[32m    313\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    314\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    315\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    316\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    317\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    318\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    319\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    320\u001b[39m     stream=stream,\n\u001b[32m    321\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    322\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    323\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    324\u001b[39m     text=response_format,\n\u001b[32m    325\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    326\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    327\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    328\u001b[39m     **extra_args,\n\u001b[32m    329\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/responses/responses.py:2259\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2224\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2257\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2258\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2260\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2261\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2262\u001b[39m             {\n\u001b[32m   2263\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2264\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2265\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2266\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2267\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2268\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2269\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2270\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2271\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2272\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2273\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2274\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2275\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2276\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2277\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2278\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2279\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2280\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2281\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2282\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2283\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2284\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2285\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2286\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2287\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2290\u001b[39m             },\n\u001b[32m   2291\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2292\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2294\u001b[39m         ),\n\u001b[32m   2295\u001b[39m         options=make_request_options(\n\u001b[32m   2296\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2297\u001b[39m         ),\n\u001b[32m   2298\u001b[39m         cast_to=Response,\n\u001b[32m   2299\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2300\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}. (request_id: req_47f173aea44c303c8de260fa6e775862)\n",
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: your-api-key. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def example_contradiction():\n",
    "    # A prompt with contradictory instructions\n",
    "    prompt = \"\"\"Quick-Start Card — Product Parser\n",
    "\n",
    "Goal  \n",
    "Digest raw HTML of an e-commerce product detail page and emit **concise, minified JSON** describing the item.\n",
    "\n",
    "**Required fields:**  \n",
    "name | brand | sku | price.value | price.currency | images[] | sizes[] | materials[] | care_instructions | features[]\n",
    "\n",
    "**Extraction priority:**  \n",
    "1. schema.org/JSON-LD blocks  \n",
    "2. <meta> & microdata tags  \n",
    "3. Visible DOM fallback (class hints: \"product-name\", \"price\")\n",
    "\n",
    "** Rules:**  \n",
    "- If *any* required field is missing, short-circuit with: `{\"error\": \"FIELD_MISSING:<field>\"}`.\n",
    "- Prices: Numeric with dot decimal; strip non-digits (e.g., \"1.299,00 EUR\" → 1299.00 + \"EUR\").\n",
    "- Deduplicate images differing only by query string. Keep ≤10 best-res.\n",
    "- Sizes: Ensure unit tag (\"EU\", \"US\") and ascending sort.\n",
    "- Materials: Title-case and collapse synonyms (e.g., \"polyester 100%\" → \"Polyester\").\n",
    "\n",
    "**Sample skeleton (minified):**\n",
    "```json\n",
    "{\"name\":\"\",\"brand\":\"\",\"sku\":\"\",\"price\":{\"value\":0,\"currency\":\"USD\"},\"images\":[\"\"],\"sizes\":[],\"materials\":[],\"care_instructions\":\"\",\"features\":[]}\n",
    "Note: It is acceptable to output null for any missing field instead of an error ###\"\"\"\n",
    "    \n",
    "    result = await optimize_prompt_parallel(prompt, [])\n",
    "    \n",
    "    # Display the results\n",
    "    if result[\"contradiction_issues\"]:\n",
    "        print(\"Contradiction issues:\")\n",
    "        print(result[\"contradiction_issues\"])\n",
    "        print()\n",
    "        \n",
    "    print(\"Optimized prompt:\")\n",
    "    print(result[\"new_developer_message\"])\n",
    "    \n",
    "# Run the example\n",
    "await example_contradiction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates how the system can detect and resolve critical contradictions that could lead to inconsistent outputs or confusion for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Fixing Inconsistencies Between Prompt and Few-Shot Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def example_fewshot_fix():\n",
    "    prompt = \"Respond **only** with JSON using keys `city` (string) and `population` (integer).\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Largest US city?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"New York City\"},\n",
    "        {\"role\": \"user\", \"content\": \"Largest UK city?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"{\\\"city\\\":\\\"London\\\",\\\"population\\\":9541000}\"}\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    print(\"Few-shot examples before optimization:\")\n",
    "    print(f\"User: {messages[0]['content']}\")\n",
    "    print(f\"Assistant: {messages[1]['content']}\")\n",
    "    print(f\"User: {messages[2]['content']}\")\n",
    "    print(f\"Assistant: {messages[3]['content']}\")\n",
    "    print()\n",
    "    \n",
    "    # Call the optimization API\n",
    "    result = await optimize_prompt_parallel(prompt, [ChatMessage(**m) for m in messages])\n",
    "    \n",
    "    # Display the results\n",
    "    if result[\"few_shot_contradiction_issues\"]:\n",
    "        print(\"Inconsistency found:\", result[\"few_shot_contradiction_issues\"])\n",
    "        print()\n",
    "    \n",
    "    # Show the optimized few-shot examples\n",
    "    optimized_messages = result[\"new_messages\"]\n",
    "    print(\"Few-shot examples after optimization:\")\n",
    "    print(f\"User: {optimized_messages[0]['content']}\")\n",
    "    print(f\"Assistant: {optimized_messages[1]['content']}\")\n",
    "    print(f\"User: {optimized_messages[2]['content']}\")\n",
    "    print(f\"Assistant: {optimized_messages[3]['content']}\")\n",
    "    \n",
    "# Run the example\n",
    "await example_fewshot_fix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly important because few-shot examples have a strong influence on how models respond. If examples don't follow the stated rules, the model may learn to ignore those rules in favor of mimicking the examples. By ensuring consistency between the prompt instructions and examples, the optimization system creats a more reliable prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Clarifying Formats in a Longer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "async def example_format_issue():\n",
    "    # A prompt with unclear or inconsistent formatting instructions\n",
    "    prompt = \"\"\"Task → Translate dense patent claims into 200-word lay summaries with a glossary.\n",
    "\n",
    "Operating Steps:\n",
    "1. Split the claim at semicolons, \"wherein\", or numbered sub-clauses.\n",
    "2. For each chunk:\n",
    "   a) Identify its purpose.\n",
    "   b) Replace technical nouns with everyday analogies.\n",
    "   c) Keep quantitative limits intact (e.g., \"≥150 C\").\n",
    "3. Flag uncommon science terms with asterisks, and later define them.\n",
    "4. Re-assemble into a flowing paragraph; do **not** broaden or narrow the claim’s scope.\n",
    "5. Omit boilerplate if its removal does not alter legal meaning.\n",
    "\n",
    "Output should follow a Markdown template:\n",
    "- A summary section.\n",
    "- A glossary section with the marked terms and their definitions.\n",
    "\n",
    "Corner Cases:\n",
    "- If the claim is over 5 kB, respond with CLAIM_TOO_LARGE.\n",
    "- If claim text is already plain English, skip glossary and state no complex terms detected.\n",
    "\n",
    "Remember: You are *not* providing legal advice—this is for internal comprehension only.\"\"\"\n",
    "\n",
    "    # Call the optimization API to check for format issues\n",
    "    result = await optimize_prompt_parallel(prompt, [])\n",
    "\n",
    "    # Display the results\n",
    "    if result.get(\"format_issues\"):\n",
    "        print(\"Format issues found:\", result[\"format_issues\"])\n",
    "        print()\n",
    "\n",
    "    print(\"Optimized prompt:\")\n",
    "    print(result[\"new_developer_message\"])\n",
    "\n",
    "# Run the example\n",
    "await example_format_issue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example highlights how the format checker identifies and resolves ambiguous format specifications. The prompt requested a Markdown output and the optimization flow significantly improved these format specifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
